In this directory, I investigated the effects of using a new nonlinearity with
the potential to speed up neural network training process.  As the nonlinearity
is essentially RELU with an upper bound, I call it **"Upper ReLU"**.

What is Upper ReLU?
-----------------------
EXPLANATION OF THE TECHNICALS, INCLUDING THE FORMULA.

How did I find this nonlinearity?
-------------------------------------
DESCRIPTION OF THE PROCESS THAT LED ME TO UPPER RELU.

Initial Findings
----------------
After I figured out its potential, I ran some quick initial tests on Upper ReLU.
Those results are stored in the [InitialFindings](./InitialFindings) folder. 
